{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt_path, transform=None, target_transform=None):\n",
    "        fh = open(txt_path, 'r')\n",
    "        imgs = []\n",
    "        for line in fh:\n",
    "            line = line.rstrip()\n",
    "            words = line.split()\n",
    "            imgs.append((words[0], int(words[1])))\n",
    "            self.imgs = imgs\n",
    "            self.transform = transform\n",
    "            self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn, label = self.imgs[index]\n",
    "        #img = Image.open(fn).convert('RGB')\n",
    "        img = Image.open(fn)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.data[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "# 修改数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载FER2013数据集的CSV文件\n",
    "data = pd.read_csv('fer2013.csv')\n",
    "\n",
    "# 数据预处理和加载\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    emotion = data['emotion'][i]\n",
    "    pixels = data['pixels'][i].split()\n",
    "    image = Image.new('L', (48, 48))  # 创建灰度图像对象\n",
    "    image.putdata([int(p) for p in pixels])  # 将像素值填充到图像中\n",
    "\n",
    "    if data['Usage'][i] == 'Training':\n",
    "        train_data.append((image, emotion))\n",
    "    elif data['Usage'][i] == 'PublicTest':\n",
    "        test_data.append((image, emotion))\n",
    "    elif data['Usage'][i] == 'PrivateTest':\n",
    "        test_data.append((image, emotion))\n",
    "\n",
    "# 创建训练数据集和测试数据集\n",
    "train_dataset = CustomDataset(train_data, transform=transform )\n",
    "test_dataset = CustomDataset(test_data, transform=transform )\n",
    "\n",
    "# 创建训练数据集和测试数据集的加载器\n",
    "train_loader =  DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader =  DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3、搭建 LeNet-5 神经网络结构\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.maxpool2 = nn.MaxPool2d(2, 2)\n",
    "        #self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc1 = nn.Linear(1296, 120) \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    # 定义前向传播\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        #x = x.view(-1, 16*5*5)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4、将定义好的网络结构搭载到 GPU/CPU，并定义优化器\n",
    "\n",
    "# 创建模型，部署gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet().to(device)\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5、定义训练函数\n",
    "Loss = []\n",
    "Accuracy = []\n",
    "def train_runner(model, device, trainloader, optimizer, epoch):\n",
    "    # 训练模型, 启用 BatchNormalization 和 Dropout, 将BatchNormalization和Dropout置为True\n",
    "    model.train()\n",
    "    total = 0\n",
    "    correct = 0.0\n",
    "\n",
    "    # enumerate迭代已加载的数据集,同时获取数据和数据下标\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        # 把模型部署到device上\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # 初始化梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 保存训练结果\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失和\n",
    "        #多分类情况通常使用cross_entropy(交叉熵损失函数), 而对于二分类问题, 通常使用sigmod\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        # 获取最大概率的预测结果\n",
    "        # dim=1表示返回每一行的最大值对应的列下标\n",
    "        predict = outputs.argmax(dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predict == labels).sum().item()\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            # loss.item()表示当前loss的数值\n",
    "            print(\"Train Epoch{} \\t Loss: {:.6f}, accuracy: {:.6f}%\".format(\n",
    "                epoch, loss.item(), 100*(correct/total)))\n",
    "            Loss.append(loss.item())\n",
    "            Accuracy.append(correct/total)\n",
    "    return loss.item(), correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6、定义测试函数\n",
    "\n",
    "def test_runner(model, device, testloader):\n",
    "    #模型验证, 必须要写, 否则只要有输入数据, 即使不训练, 它也会改变权值\n",
    "    # 因为调用eval()将不启用 BatchNormalization 和 Dropout, BatchNormalization和Dropout置为False\n",
    "    model.eval()\n",
    "    #统计模型正确率, 设置初始值\n",
    "    correct = 0.0\n",
    "    test_loss = 0.0\n",
    "    total = 0\n",
    "    #torch.no_grad将不会计算梯度, 也不会进行反向传播\n",
    "    with torch.no_grad():\n",
    "        for data, label in testloader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, label).item()\n",
    "            predict = output.argmax(dim=1)\n",
    "            # 计算正确数量\n",
    "            total += label.size(0)\n",
    "            correct += (predict == label).sum().item()\n",
    "        # 计算损失值\n",
    "        print(\"test_avarage_loss: {:.6f}, accuracy: {:.6f}%\".format(\n",
    "            test_loss/total, 100*(correct/total)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time 2023-07-15 11:07:00\n",
      "Train Epoch1 \t Loss: 2.275713, accuracy: 6.250000%\n",
      "Train Epoch1 \t Loss: 1.826513, accuracy: 22.091584%\n",
      "Train Epoch1 \t Loss: 1.866823, accuracy: 23.554104%\n",
      "Train Epoch1 \t Loss: 1.609335, accuracy: 25.342608%\n",
      "Train Epoch1 \t Loss: 1.892802, accuracy: 27.509352%\n",
      "Train Epoch1 \t Loss: 1.536821, accuracy: 29.029441%\n",
      "Train Epoch1 \t Loss: 1.859015, accuracy: 30.246464%\n",
      "Train Epoch1 \t Loss: 1.412086, accuracy: 31.290121%\n",
      "Train Epoch1 \t Loss: 1.407089, accuracy: 32.307272%\n",
      "test_avarage_loss: 0.047783, accuracy: 41.111730%\n",
      "end_time:  2023-07-15 11:07:05 \n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# 7、运行\n",
    "# 调用\n",
    "epoch = 1\n",
    "Loss = []\n",
    "Accuracy = []\n",
    "for epoch in range(1, epoch+1):\n",
    "    print(\"start_time\", time.strftime(\n",
    "        '%Y-%m-%d %H:%M:%S', time.localtime(time.time())))\n",
    "    loss, acc = train_runner(model, device, train_loader, optimizer, epoch)\n",
    "    Loss.append(loss)\n",
    "    Accuracy.append(acc)\n",
    "    test_runner(model, device, test_loader)\n",
    "    print(\"end_time: \", time.strftime(\n",
    "        '%Y-%m-%d %H:%M:%S', time.localtime(time.time())), '\\n')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "Loss = np.asarray(Loss)\n",
    "Accuracy = np.asarray(Accuracy)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(Loss)\n",
    "plt.title('Loss')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'model_LeNet5.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed94aefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Administrator\\\\FTP\\\\20230715'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e523ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc095b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Reshape, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):\n",
    "        super(Residual, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, padding=1, stride=stride\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, padding=1\n",
    "        )\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=1, stride=stride\n",
    "            )\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        return F.relu(Y + X)\n",
    "\n",
    "\n",
    "def resnet_block(in_channels, out_channels, num_residuals, first_block=False):\n",
    "    if first_block:\n",
    "        assert in_channels == out_channels\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(\n",
    "                Residual(in_channels, out_channels, use_1x1conv=True, stride=2)\n",
    "            )\n",
    "        else:\n",
    "            blk.append(Residual(out_channels, out_channels))\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "661134c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.data[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c8dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a433d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载FER2013数据集的CSV文件\n",
    "data = pd.read_csv('fer2013.csv')\n",
    "\n",
    "# 数据预处理和加载\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    emotion = data['emotion'][i]\n",
    "    pixels = data['pixels'][i].split()\n",
    "    image = Image.new('L', (48, 48))  # 创建灰度图像对象\n",
    "    image.putdata([int(p) for p in pixels])  # 将像素值填充到图像中\n",
    "\n",
    "    if data['Usage'][i] == 'Training':\n",
    "        train_data.append((image, emotion))\n",
    "    elif data['Usage'][i] == 'PublicTest':\n",
    "        test_data.append((image, emotion))\n",
    "    elif data['Usage'][i] == 'PrivateTest':\n",
    "        test_data.append((image, emotion))\n",
    "\n",
    "# 创建训练数据集和测试数据集\n",
    "train_dataset = CustomDataset(train_data, transform=transform)\n",
    "test_dataset = CustomDataset(test_data, transform=transform)\n",
    "\n",
    "# 创建训练数据集和测试数据集的加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed49218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d2437ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")\n",
    "\n",
    "model.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\n",
    "model.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n",
    "model.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n",
    "model.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\n",
    "model.add_module(\"global_avg_pool\", GlobalAvgPool2d())\n",
    "model.add_module(\"fc\", nn.Sequential(Reshape(), nn.Linear(512, 7)))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df03ed4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Train Loss: 1.7762, Train Accuracy: 0.2756, Test Loss: 2.2234, Test Accuracy: 0.2777\n",
      "Epoch [2/300], Train Loss: 1.5594, Train Accuracy: 0.3822, Test Loss: 1.5650, Test Accuracy: 0.3880\n",
      "Epoch [3/300], Train Loss: 1.4302, Train Accuracy: 0.4392, Test Loss: 1.4283, Test Accuracy: 0.4446\n",
      "Epoch [4/300], Train Loss: 1.3414, Train Accuracy: 0.4809, Test Loss: 1.3765, Test Accuracy: 0.4688\n",
      "Epoch [5/300], Train Loss: 1.2737, Train Accuracy: 0.5099, Test Loss: 1.4633, Test Accuracy: 0.4329\n",
      "Epoch [6/300], Train Loss: 1.2165, Train Accuracy: 0.5361, Test Loss: 1.3171, Test Accuracy: 0.4909\n",
      "Epoch [7/300], Train Loss: 1.1532, Train Accuracy: 0.5640, Test Loss: 1.2624, Test Accuracy: 0.5206\n",
      "Epoch [8/300], Train Loss: 1.0870, Train Accuracy: 0.5911, Test Loss: 1.3292, Test Accuracy: 0.4884\n",
      "Epoch [9/300], Train Loss: 1.0118, Train Accuracy: 0.6217, Test Loss: 1.3350, Test Accuracy: 0.5131\n",
      "Epoch [10/300], Train Loss: 0.9283, Train Accuracy: 0.6580, Test Loss: 1.3164, Test Accuracy: 0.5385\n",
      "Epoch [11/300], Train Loss: 0.8391, Train Accuracy: 0.6961, Test Loss: 1.3062, Test Accuracy: 0.5467\n",
      "Epoch [12/300], Train Loss: 0.7469, Train Accuracy: 0.7310, Test Loss: 1.4043, Test Accuracy: 0.5435\n",
      "Epoch [13/300], Train Loss: 0.6513, Train Accuracy: 0.7679, Test Loss: 1.4778, Test Accuracy: 0.5500\n",
      "Epoch [14/300], Train Loss: 0.5776, Train Accuracy: 0.7953, Test Loss: 1.5328, Test Accuracy: 0.5401\n",
      "Epoch [15/300], Train Loss: 0.4972, Train Accuracy: 0.8278, Test Loss: 1.5618, Test Accuracy: 0.5538\n",
      "Epoch [16/300], Train Loss: 0.4285, Train Accuracy: 0.8530, Test Loss: 1.8148, Test Accuracy: 0.5259\n",
      "Epoch [17/300], Train Loss: 0.3774, Train Accuracy: 0.8684, Test Loss: 1.9207, Test Accuracy: 0.5401\n",
      "Epoch [18/300], Train Loss: 0.3320, Train Accuracy: 0.8867, Test Loss: 1.9182, Test Accuracy: 0.5400\n",
      "Epoch [19/300], Train Loss: 0.2975, Train Accuracy: 0.8978, Test Loss: 2.0015, Test Accuracy: 0.5492\n",
      "Epoch [20/300], Train Loss: 0.2677, Train Accuracy: 0.9108, Test Loss: 2.0435, Test Accuracy: 0.5339\n",
      "Epoch [21/300], Train Loss: 0.2556, Train Accuracy: 0.9128, Test Loss: 2.1771, Test Accuracy: 0.5476\n",
      "Epoch [22/300], Train Loss: 0.2269, Train Accuracy: 0.9246, Test Loss: 2.1307, Test Accuracy: 0.5436\n",
      "Epoch [23/300], Train Loss: 0.2171, Train Accuracy: 0.9283, Test Loss: 2.2149, Test Accuracy: 0.5457\n",
      "Epoch [24/300], Train Loss: 0.1898, Train Accuracy: 0.9379, Test Loss: 2.3407, Test Accuracy: 0.5355\n",
      "Epoch [25/300], Train Loss: 0.2083, Train Accuracy: 0.9306, Test Loss: 2.3324, Test Accuracy: 0.5428\n",
      "Epoch [26/300], Train Loss: 0.1793, Train Accuracy: 0.9395, Test Loss: 2.3178, Test Accuracy: 0.5529\n",
      "Epoch [27/300], Train Loss: 0.1580, Train Accuracy: 0.9475, Test Loss: 2.3743, Test Accuracy: 0.5270\n",
      "Epoch [28/300], Train Loss: 0.1555, Train Accuracy: 0.9475, Test Loss: 2.4753, Test Accuracy: 0.5573\n",
      "Epoch [29/300], Train Loss: 0.1517, Train Accuracy: 0.9502, Test Loss: 2.3099, Test Accuracy: 0.5471\n",
      "Epoch [30/300], Train Loss: 0.1431, Train Accuracy: 0.9528, Test Loss: 2.5756, Test Accuracy: 0.5497\n",
      "Epoch [31/300], Train Loss: 0.1547, Train Accuracy: 0.9496, Test Loss: 2.4699, Test Accuracy: 0.5398\n",
      "Epoch [32/300], Train Loss: 0.1397, Train Accuracy: 0.9527, Test Loss: 2.3081, Test Accuracy: 0.5391\n",
      "Epoch [33/300], Train Loss: 0.1342, Train Accuracy: 0.9549, Test Loss: 2.4945, Test Accuracy: 0.5358\n",
      "Epoch [34/300], Train Loss: 0.1312, Train Accuracy: 0.9561, Test Loss: 2.4571, Test Accuracy: 0.5535\n",
      "Epoch [35/300], Train Loss: 0.1352, Train Accuracy: 0.9551, Test Loss: 2.2903, Test Accuracy: 0.5414\n",
      "Epoch [36/300], Train Loss: 0.1245, Train Accuracy: 0.9583, Test Loss: 2.6281, Test Accuracy: 0.5517\n",
      "Epoch [37/300], Train Loss: 0.1282, Train Accuracy: 0.9568, Test Loss: 2.5190, Test Accuracy: 0.5458\n",
      "Epoch [38/300], Train Loss: 0.1225, Train Accuracy: 0.9598, Test Loss: 2.4679, Test Accuracy: 0.5465\n",
      "Epoch [39/300], Train Loss: 0.1044, Train Accuracy: 0.9662, Test Loss: 2.6930, Test Accuracy: 0.5465\n",
      "Epoch [40/300], Train Loss: 0.0989, Train Accuracy: 0.9674, Test Loss: 2.4805, Test Accuracy: 0.5524\n",
      "Epoch [41/300], Train Loss: 0.1093, Train Accuracy: 0.9637, Test Loss: 2.7417, Test Accuracy: 0.5327\n",
      "Epoch [42/300], Train Loss: 0.1149, Train Accuracy: 0.9625, Test Loss: 2.5973, Test Accuracy: 0.5581\n",
      "Epoch [43/300], Train Loss: 0.1039, Train Accuracy: 0.9663, Test Loss: 2.7309, Test Accuracy: 0.5549\n",
      "Epoch [44/300], Train Loss: 0.1089, Train Accuracy: 0.9643, Test Loss: 2.7086, Test Accuracy: 0.5575\n",
      "Epoch [45/300], Train Loss: 0.1131, Train Accuracy: 0.9615, Test Loss: 2.5252, Test Accuracy: 0.5614\n",
      "Epoch [46/300], Train Loss: 0.0983, Train Accuracy: 0.9681, Test Loss: 2.6760, Test Accuracy: 0.5525\n",
      "Epoch [47/300], Train Loss: 0.0835, Train Accuracy: 0.9724, Test Loss: 2.7737, Test Accuracy: 0.5513\n",
      "Epoch [48/300], Train Loss: 0.0983, Train Accuracy: 0.9672, Test Loss: 2.6211, Test Accuracy: 0.5531\n",
      "Epoch [49/300], Train Loss: 0.1186, Train Accuracy: 0.9616, Test Loss: 2.4284, Test Accuracy: 0.5531\n",
      "Epoch [50/300], Train Loss: 0.0858, Train Accuracy: 0.9730, Test Loss: 2.6373, Test Accuracy: 0.5464\n",
      "Epoch [51/300], Train Loss: 0.0724, Train Accuracy: 0.9761, Test Loss: 2.8210, Test Accuracy: 0.5350\n",
      "Epoch [52/300], Train Loss: 0.0879, Train Accuracy: 0.9717, Test Loss: 2.8274, Test Accuracy: 0.5387\n",
      "Epoch [53/300], Train Loss: 0.0877, Train Accuracy: 0.9720, Test Loss: 2.7170, Test Accuracy: 0.5532\n",
      "Epoch [54/300], Train Loss: 0.0873, Train Accuracy: 0.9720, Test Loss: 2.7184, Test Accuracy: 0.5397\n",
      "Epoch [55/300], Train Loss: 0.1011, Train Accuracy: 0.9669, Test Loss: 2.7710, Test Accuracy: 0.5568\n",
      "Epoch [56/300], Train Loss: 0.0847, Train Accuracy: 0.9737, Test Loss: 2.7385, Test Accuracy: 0.5561\n",
      "Epoch [57/300], Train Loss: 0.0758, Train Accuracy: 0.9758, Test Loss: 2.8086, Test Accuracy: 0.5556\n",
      "Epoch [58/300], Train Loss: 0.0883, Train Accuracy: 0.9714, Test Loss: 2.8870, Test Accuracy: 0.5561\n",
      "Epoch [59/300], Train Loss: 0.0947, Train Accuracy: 0.9706, Test Loss: 2.7110, Test Accuracy: 0.5527\n",
      "Epoch [60/300], Train Loss: 0.0719, Train Accuracy: 0.9778, Test Loss: 3.0623, Test Accuracy: 0.5593\n",
      "Epoch [61/300], Train Loss: 0.0724, Train Accuracy: 0.9762, Test Loss: 2.8563, Test Accuracy: 0.5580\n",
      "Epoch [62/300], Train Loss: 0.0735, Train Accuracy: 0.9752, Test Loss: 2.8282, Test Accuracy: 0.5529\n",
      "Epoch [63/300], Train Loss: 0.0782, Train Accuracy: 0.9743, Test Loss: 2.9048, Test Accuracy: 0.5458\n",
      "Epoch [64/300], Train Loss: 0.0713, Train Accuracy: 0.9767, Test Loss: 2.8600, Test Accuracy: 0.5518\n",
      "Epoch [65/300], Train Loss: 0.0708, Train Accuracy: 0.9760, Test Loss: 2.7254, Test Accuracy: 0.5474\n",
      "Epoch [66/300], Train Loss: 0.0740, Train Accuracy: 0.9765, Test Loss: 2.9112, Test Accuracy: 0.5524\n",
      "Epoch [67/300], Train Loss: 0.0733, Train Accuracy: 0.9785, Test Loss: 3.1502, Test Accuracy: 0.5400\n",
      "Epoch [68/300], Train Loss: 0.0994, Train Accuracy: 0.9700, Test Loss: 2.7933, Test Accuracy: 0.5479\n",
      "Epoch [69/300], Train Loss: 0.0591, Train Accuracy: 0.9807, Test Loss: 2.8846, Test Accuracy: 0.5478\n",
      "Epoch [70/300], Train Loss: 0.0573, Train Accuracy: 0.9812, Test Loss: 3.0888, Test Accuracy: 0.5571\n",
      "Epoch [71/300], Train Loss: 0.0776, Train Accuracy: 0.9744, Test Loss: 3.0380, Test Accuracy: 0.5450\n",
      "Epoch [72/300], Train Loss: 0.0718, Train Accuracy: 0.9760, Test Loss: 3.0056, Test Accuracy: 0.5522\n",
      "Epoch [73/300], Train Loss: 0.0511, Train Accuracy: 0.9832, Test Loss: 2.9827, Test Accuracy: 0.5605\n",
      "Epoch [74/300], Train Loss: 0.0712, Train Accuracy: 0.9767, Test Loss: 2.8267, Test Accuracy: 0.5488\n",
      "Epoch [75/300], Train Loss: 0.0958, Train Accuracy: 0.9710, Test Loss: 3.1412, Test Accuracy: 0.5463\n",
      "Epoch [76/300], Train Loss: 0.0540, Train Accuracy: 0.9823, Test Loss: 3.0749, Test Accuracy: 0.5595\n",
      "Epoch [77/300], Train Loss: 0.0564, Train Accuracy: 0.9815, Test Loss: 2.9019, Test Accuracy: 0.5499\n",
      "Epoch [78/300], Train Loss: 0.0650, Train Accuracy: 0.9791, Test Loss: 2.9850, Test Accuracy: 0.5493\n",
      "Epoch [79/300], Train Loss: 0.0706, Train Accuracy: 0.9772, Test Loss: 2.9493, Test Accuracy: 0.5492\n",
      "Epoch [80/300], Train Loss: 0.0488, Train Accuracy: 0.9841, Test Loss: 3.0278, Test Accuracy: 0.5422\n",
      "Epoch [81/300], Train Loss: 0.0710, Train Accuracy: 0.9762, Test Loss: 3.0827, Test Accuracy: 0.5443\n",
      "Epoch [82/300], Train Loss: 0.0843, Train Accuracy: 0.9744, Test Loss: 2.9622, Test Accuracy: 0.5549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/300], Train Loss: 0.0346, Train Accuracy: 0.9884, Test Loss: 3.0257, Test Accuracy: 0.5553\n",
      "Epoch [84/300], Train Loss: 0.0789, Train Accuracy: 0.9755, Test Loss: 2.6451, Test Accuracy: 0.5539\n",
      "Epoch [85/300], Train Loss: 0.0652, Train Accuracy: 0.9801, Test Loss: 2.9233, Test Accuracy: 0.5465\n",
      "Epoch [86/300], Train Loss: 0.0546, Train Accuracy: 0.9824, Test Loss: 2.9796, Test Accuracy: 0.5502\n",
      "Epoch [87/300], Train Loss: 0.0581, Train Accuracy: 0.9807, Test Loss: 3.2283, Test Accuracy: 0.5588\n",
      "Epoch [88/300], Train Loss: 0.0493, Train Accuracy: 0.9836, Test Loss: 3.0621, Test Accuracy: 0.5613\n",
      "Epoch [89/300], Train Loss: 0.0686, Train Accuracy: 0.9781, Test Loss: 2.8990, Test Accuracy: 0.5541\n",
      "Epoch [90/300], Train Loss: 0.0539, Train Accuracy: 0.9821, Test Loss: 2.9971, Test Accuracy: 0.5529\n",
      "Epoch [91/300], Train Loss: 0.0532, Train Accuracy: 0.9833, Test Loss: 2.9384, Test Accuracy: 0.5493\n",
      "Epoch [92/300], Train Loss: 0.0752, Train Accuracy: 0.9769, Test Loss: 2.9202, Test Accuracy: 0.5624\n",
      "Epoch [93/300], Train Loss: 0.0348, Train Accuracy: 0.9883, Test Loss: 3.0463, Test Accuracy: 0.5476\n",
      "Epoch [94/300], Train Loss: 0.0706, Train Accuracy: 0.9786, Test Loss: 2.8386, Test Accuracy: 0.5545\n",
      "Epoch [95/300], Train Loss: 0.0578, Train Accuracy: 0.9817, Test Loss: 3.1177, Test Accuracy: 0.5656\n",
      "Epoch [96/300], Train Loss: 0.0543, Train Accuracy: 0.9835, Test Loss: 2.8660, Test Accuracy: 0.5591\n",
      "Epoch [97/300], Train Loss: 0.0572, Train Accuracy: 0.9812, Test Loss: 2.9541, Test Accuracy: 0.5531\n",
      "Epoch [98/300], Train Loss: 0.0415, Train Accuracy: 0.9857, Test Loss: 2.9971, Test Accuracy: 0.5563\n",
      "Epoch [99/300], Train Loss: 0.0444, Train Accuracy: 0.9854, Test Loss: 3.0618, Test Accuracy: 0.5554\n",
      "Epoch [100/300], Train Loss: 0.0664, Train Accuracy: 0.9784, Test Loss: 3.1974, Test Accuracy: 0.5578\n",
      "Epoch [101/300], Train Loss: 0.0697, Train Accuracy: 0.9785, Test Loss: 3.0555, Test Accuracy: 0.5570\n",
      "Epoch [102/300], Train Loss: 0.0415, Train Accuracy: 0.9865, Test Loss: 3.0026, Test Accuracy: 0.5589\n",
      "Epoch [103/300], Train Loss: 0.0554, Train Accuracy: 0.9825, Test Loss: 3.0416, Test Accuracy: 0.5563\n",
      "Epoch [104/300], Train Loss: 0.0381, Train Accuracy: 0.9871, Test Loss: 3.0352, Test Accuracy: 0.5598\n",
      "Epoch [105/300], Train Loss: 0.0575, Train Accuracy: 0.9812, Test Loss: 3.0455, Test Accuracy: 0.5610\n",
      "Epoch [106/300], Train Loss: 0.0849, Train Accuracy: 0.9727, Test Loss: 2.6310, Test Accuracy: 0.5449\n",
      "Epoch [107/300], Train Loss: 0.0567, Train Accuracy: 0.9814, Test Loss: 3.0852, Test Accuracy: 0.5599\n",
      "Epoch [108/300], Train Loss: 0.0563, Train Accuracy: 0.9815, Test Loss: 2.9943, Test Accuracy: 0.5639\n",
      "Epoch [109/300], Train Loss: 0.0387, Train Accuracy: 0.9875, Test Loss: 2.9892, Test Accuracy: 0.5656\n",
      "Epoch [110/300], Train Loss: 0.0335, Train Accuracy: 0.9884, Test Loss: 3.2228, Test Accuracy: 0.5626\n",
      "Epoch [111/300], Train Loss: 0.0525, Train Accuracy: 0.9840, Test Loss: 3.0881, Test Accuracy: 0.5553\n",
      "Epoch [112/300], Train Loss: 0.0853, Train Accuracy: 0.9753, Test Loss: 2.9797, Test Accuracy: 0.5612\n",
      "Epoch [113/300], Train Loss: 0.0503, Train Accuracy: 0.9842, Test Loss: 2.9877, Test Accuracy: 0.5560\n",
      "Epoch [114/300], Train Loss: 0.0363, Train Accuracy: 0.9880, Test Loss: 3.1993, Test Accuracy: 0.5610\n",
      "Epoch [115/300], Train Loss: 0.0591, Train Accuracy: 0.9815, Test Loss: 3.1676, Test Accuracy: 0.5529\n",
      "Epoch [116/300], Train Loss: 0.0740, Train Accuracy: 0.9803, Test Loss: 3.1527, Test Accuracy: 0.5503\n",
      "Epoch [117/300], Train Loss: 0.0486, Train Accuracy: 0.9845, Test Loss: 3.0183, Test Accuracy: 0.5432\n",
      "Epoch [118/300], Train Loss: 0.0373, Train Accuracy: 0.9884, Test Loss: 3.3893, Test Accuracy: 0.5587\n",
      "Epoch [119/300], Train Loss: 0.0511, Train Accuracy: 0.9831, Test Loss: 3.1330, Test Accuracy: 0.5683\n",
      "Epoch [120/300], Train Loss: 0.0442, Train Accuracy: 0.9860, Test Loss: 3.1009, Test Accuracy: 0.5561\n",
      "Epoch [121/300], Train Loss: 0.0458, Train Accuracy: 0.9868, Test Loss: 3.0492, Test Accuracy: 0.5548\n",
      "Epoch [122/300], Train Loss: 0.0407, Train Accuracy: 0.9871, Test Loss: 3.2723, Test Accuracy: 0.5577\n",
      "Epoch [123/300], Train Loss: 0.0442, Train Accuracy: 0.9859, Test Loss: 3.1005, Test Accuracy: 0.5532\n",
      "Epoch [124/300], Train Loss: 0.0495, Train Accuracy: 0.9845, Test Loss: 3.0351, Test Accuracy: 0.5483\n",
      "Epoch [125/300], Train Loss: 0.0442, Train Accuracy: 0.9850, Test Loss: 3.1384, Test Accuracy: 0.5585\n",
      "Epoch [126/300], Train Loss: 0.0584, Train Accuracy: 0.9819, Test Loss: 3.0157, Test Accuracy: 0.5642\n",
      "Epoch [127/300], Train Loss: 0.0457, Train Accuracy: 0.9853, Test Loss: 3.1425, Test Accuracy: 0.5600\n",
      "Epoch [128/300], Train Loss: 0.0289, Train Accuracy: 0.9897, Test Loss: 3.4392, Test Accuracy: 0.5690\n",
      "Epoch [129/300], Train Loss: 0.0572, Train Accuracy: 0.9814, Test Loss: 2.9484, Test Accuracy: 0.5508\n",
      "Epoch [130/300], Train Loss: 0.0675, Train Accuracy: 0.9795, Test Loss: 2.9592, Test Accuracy: 0.5568\n",
      "Epoch [131/300], Train Loss: 0.0762, Train Accuracy: 0.9808, Test Loss: 3.2460, Test Accuracy: 0.5543\n",
      "Epoch [132/300], Train Loss: 0.0275, Train Accuracy: 0.9915, Test Loss: 3.2065, Test Accuracy: 0.5624\n",
      "Epoch [133/300], Train Loss: 0.0377, Train Accuracy: 0.9879, Test Loss: 3.1446, Test Accuracy: 0.5582\n",
      "Epoch [134/300], Train Loss: 0.0544, Train Accuracy: 0.9835, Test Loss: 3.0869, Test Accuracy: 0.5646\n",
      "Epoch [135/300], Train Loss: 0.0278, Train Accuracy: 0.9903, Test Loss: 3.2394, Test Accuracy: 0.5567\n",
      "Epoch [136/300], Train Loss: 0.0322, Train Accuracy: 0.9887, Test Loss: 3.0819, Test Accuracy: 0.5556\n",
      "Epoch [137/300], Train Loss: 0.0397, Train Accuracy: 0.9870, Test Loss: 3.3904, Test Accuracy: 0.5475\n",
      "Epoch [138/300], Train Loss: 0.0754, Train Accuracy: 0.9775, Test Loss: 2.9796, Test Accuracy: 0.5389\n",
      "Epoch [139/300], Train Loss: 0.1078, Train Accuracy: 0.9710, Test Loss: 3.0641, Test Accuracy: 0.5563\n",
      "Epoch [140/300], Train Loss: 0.0429, Train Accuracy: 0.9881, Test Loss: 2.9889, Test Accuracy: 0.5554\n",
      "Epoch [141/300], Train Loss: 0.0257, Train Accuracy: 0.9916, Test Loss: 3.2636, Test Accuracy: 0.5587\n",
      "Epoch [142/300], Train Loss: 0.0225, Train Accuracy: 0.9923, Test Loss: 3.3959, Test Accuracy: 0.5639\n",
      "Epoch [143/300], Train Loss: 0.0409, Train Accuracy: 0.9871, Test Loss: 3.2401, Test Accuracy: 0.5515\n",
      "Epoch [144/300], Train Loss: 0.0388, Train Accuracy: 0.9872, Test Loss: 3.1196, Test Accuracy: 0.5531\n",
      "Epoch [145/300], Train Loss: 0.0586, Train Accuracy: 0.9846, Test Loss: 2.9467, Test Accuracy: 0.5584\n",
      "Epoch [146/300], Train Loss: 0.0348, Train Accuracy: 0.9879, Test Loss: 3.3910, Test Accuracy: 0.5603\n",
      "Epoch [147/300], Train Loss: 0.0299, Train Accuracy: 0.9900, Test Loss: 3.2277, Test Accuracy: 0.5543\n",
      "Epoch [148/300], Train Loss: 0.0772, Train Accuracy: 0.9782, Test Loss: 2.9360, Test Accuracy: 0.5465\n",
      "Epoch [149/300], Train Loss: 0.0365, Train Accuracy: 0.9881, Test Loss: 3.3368, Test Accuracy: 0.5542\n",
      "Epoch [150/300], Train Loss: 0.0355, Train Accuracy: 0.9874, Test Loss: 3.4279, Test Accuracy: 0.5588\n",
      "Epoch [151/300], Train Loss: 0.0275, Train Accuracy: 0.9900, Test Loss: 3.4296, Test Accuracy: 0.5514\n",
      "Epoch [152/300], Train Loss: 0.0443, Train Accuracy: 0.9853, Test Loss: 3.1826, Test Accuracy: 0.5460\n",
      "Epoch [153/300], Train Loss: 0.0344, Train Accuracy: 0.9885, Test Loss: 3.2227, Test Accuracy: 0.5514\n",
      "Epoch [154/300], Train Loss: 0.0403, Train Accuracy: 0.9869, Test Loss: 3.1684, Test Accuracy: 0.5422\n",
      "Epoch [155/300], Train Loss: 0.0930, Train Accuracy: 0.9726, Test Loss: 3.1313, Test Accuracy: 0.5488\n",
      "Epoch [156/300], Train Loss: 0.0283, Train Accuracy: 0.9916, Test Loss: 3.4338, Test Accuracy: 0.5556\n",
      "Epoch [157/300], Train Loss: 0.0226, Train Accuracy: 0.9923, Test Loss: 3.5006, Test Accuracy: 0.5573\n",
      "Epoch [158/300], Train Loss: 0.0401, Train Accuracy: 0.9882, Test Loss: 3.4343, Test Accuracy: 0.5456\n",
      "Epoch [159/300], Train Loss: 0.0616, Train Accuracy: 0.9809, Test Loss: 3.0301, Test Accuracy: 0.5525\n",
      "Epoch [160/300], Train Loss: 0.0243, Train Accuracy: 0.9916, Test Loss: 3.2867, Test Accuracy: 0.5506\n",
      "Epoch [161/300], Train Loss: 0.0281, Train Accuracy: 0.9910, Test Loss: 3.2960, Test Accuracy: 0.5419\n",
      "Epoch [162/300], Train Loss: 0.0415, Train Accuracy: 0.9866, Test Loss: 3.2287, Test Accuracy: 0.5514\n",
      "Epoch [163/300], Train Loss: 0.0278, Train Accuracy: 0.9908, Test Loss: 3.7539, Test Accuracy: 0.5534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [164/300], Train Loss: 0.0343, Train Accuracy: 0.9884, Test Loss: 3.6655, Test Accuracy: 0.5573\n",
      "Epoch [165/300], Train Loss: 0.0819, Train Accuracy: 0.9772, Test Loss: 2.8442, Test Accuracy: 0.5524\n",
      "Epoch [166/300], Train Loss: 0.0246, Train Accuracy: 0.9913, Test Loss: 3.3433, Test Accuracy: 0.5542\n",
      "Epoch [167/300], Train Loss: 0.0191, Train Accuracy: 0.9934, Test Loss: 3.4937, Test Accuracy: 0.5646\n",
      "Epoch [168/300], Train Loss: 0.0335, Train Accuracy: 0.9891, Test Loss: 3.2497, Test Accuracy: 0.5435\n",
      "Epoch [169/300], Train Loss: 0.0323, Train Accuracy: 0.9892, Test Loss: 3.2981, Test Accuracy: 0.5631\n",
      "Epoch [170/300], Train Loss: 0.0246, Train Accuracy: 0.9917, Test Loss: 3.0815, Test Accuracy: 0.5607\n",
      "Epoch [171/300], Train Loss: 0.0328, Train Accuracy: 0.9894, Test Loss: 3.2011, Test Accuracy: 0.5503\n",
      "Epoch [172/300], Train Loss: 0.0586, Train Accuracy: 0.9813, Test Loss: 3.1119, Test Accuracy: 0.5548\n",
      "Epoch [173/300], Train Loss: 0.0644, Train Accuracy: 0.9816, Test Loss: 3.4103, Test Accuracy: 0.5329\n",
      "Epoch [174/300], Train Loss: 0.0628, Train Accuracy: 0.9819, Test Loss: 2.9727, Test Accuracy: 0.5359\n",
      "Epoch [175/300], Train Loss: 0.0397, Train Accuracy: 0.9875, Test Loss: 3.4315, Test Accuracy: 0.5606\n",
      "Epoch [176/300], Train Loss: 0.0312, Train Accuracy: 0.9899, Test Loss: 3.7148, Test Accuracy: 0.5575\n",
      "Epoch [177/300], Train Loss: 0.0231, Train Accuracy: 0.9925, Test Loss: 3.6326, Test Accuracy: 0.5599\n",
      "Epoch [178/300], Train Loss: 0.0296, Train Accuracy: 0.9903, Test Loss: 3.5594, Test Accuracy: 0.5599\n",
      "Epoch [179/300], Train Loss: 0.0508, Train Accuracy: 0.9839, Test Loss: 3.2756, Test Accuracy: 0.5607\n",
      "Epoch [180/300], Train Loss: 0.0289, Train Accuracy: 0.9910, Test Loss: 3.4640, Test Accuracy: 0.5578\n",
      "Epoch [181/300], Train Loss: 0.0577, Train Accuracy: 0.9823, Test Loss: 2.7496, Test Accuracy: 0.5588\n",
      "Epoch [182/300], Train Loss: 0.0804, Train Accuracy: 0.9796, Test Loss: 3.2825, Test Accuracy: 0.5638\n",
      "Epoch [183/300], Train Loss: 0.0220, Train Accuracy: 0.9927, Test Loss: 3.2723, Test Accuracy: 0.5612\n",
      "Epoch [184/300], Train Loss: 0.0231, Train Accuracy: 0.9922, Test Loss: 3.8057, Test Accuracy: 0.5532\n",
      "Epoch [185/300], Train Loss: 0.0429, Train Accuracy: 0.9863, Test Loss: 3.3913, Test Accuracy: 0.5542\n",
      "Epoch [186/300], Train Loss: 0.0249, Train Accuracy: 0.9919, Test Loss: 3.3767, Test Accuracy: 0.5543\n",
      "Epoch [187/300], Train Loss: 0.0553, Train Accuracy: 0.9821, Test Loss: 3.6862, Test Accuracy: 0.5521\n",
      "Epoch [188/300], Train Loss: 0.0400, Train Accuracy: 0.9876, Test Loss: 3.1873, Test Accuracy: 0.5471\n",
      "Epoch [189/300], Train Loss: 0.0422, Train Accuracy: 0.9857, Test Loss: 3.4838, Test Accuracy: 0.5549\n",
      "Epoch [190/300], Train Loss: 0.0278, Train Accuracy: 0.9902, Test Loss: 3.5618, Test Accuracy: 0.5563\n",
      "Epoch [191/300], Train Loss: 0.0225, Train Accuracy: 0.9925, Test Loss: 3.5763, Test Accuracy: 0.5592\n",
      "Epoch [192/300], Train Loss: 0.0358, Train Accuracy: 0.9891, Test Loss: 3.7374, Test Accuracy: 0.5557\n",
      "Epoch [193/300], Train Loss: 0.0236, Train Accuracy: 0.9917, Test Loss: 4.1503, Test Accuracy: 0.5620\n",
      "Epoch [194/300], Train Loss: 0.0553, Train Accuracy: 0.9829, Test Loss: 3.1101, Test Accuracy: 0.5566\n",
      "Epoch [195/300], Train Loss: 0.0338, Train Accuracy: 0.9900, Test Loss: 3.8012, Test Accuracy: 0.5548\n",
      "Epoch [196/300], Train Loss: 0.0468, Train Accuracy: 0.9852, Test Loss: 3.5010, Test Accuracy: 0.5733\n",
      "Epoch [197/300], Train Loss: 0.0323, Train Accuracy: 0.9901, Test Loss: 3.1265, Test Accuracy: 0.5660\n",
      "Epoch [198/300], Train Loss: 0.0275, Train Accuracy: 0.9923, Test Loss: 3.3128, Test Accuracy: 0.5532\n",
      "Epoch [199/300], Train Loss: 0.0395, Train Accuracy: 0.9892, Test Loss: 3.4030, Test Accuracy: 0.5352\n",
      "Epoch [200/300], Train Loss: 0.0487, Train Accuracy: 0.9846, Test Loss: 3.2942, Test Accuracy: 0.5591\n",
      "Epoch [201/300], Train Loss: 0.0261, Train Accuracy: 0.9916, Test Loss: 3.6052, Test Accuracy: 0.5400\n",
      "Epoch [202/300], Train Loss: 0.0646, Train Accuracy: 0.9811, Test Loss: 3.1794, Test Accuracy: 0.5667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0.0\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    train_accuracy = train_correct / len(train_dataset)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    test_loss = test_loss / len(test_dataset)\n",
    "    test_accuracy = test_correct / len(test_dataset)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "        f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22c75285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f6fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb90e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
